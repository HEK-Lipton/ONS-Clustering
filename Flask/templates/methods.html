<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>About</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
</head>
<body>
<!-- Reference:
 W3Schools, 2024, https://www.w3schools.com/bootstrap/bootstrap_navbar.asp
 Nav bar code edited from W3School cod 
-->
    <div class="container-fluid bg-secondary">
        <nav class="navbar navbar-expand-sm navbar-light bg-secondary">
            <a href="" class="navbar-brand mb-0 h3">The Census Clustered</a>
            <div class="navbar-collapse">
                <ul class="navbar-nav">
                    <li class="nav-item">
                        <a href="/" class="nav-link">Home</a>
                    </li>
                    <li class="nav-item">
                        <a href="/about" class="nav-link active">About</a>
                    </li>
                    <li class="nav-item">
                        <a href="/methods" class="nav-link">Methods</a>
                    </li>
                </ul>
            </div> 
        </nav>
    </div>
    <section class="container">
        <div class="row my-4 gy-4">
            <div class="col-12 bg-secondary rounded">
                <p>
                    <h2>What are different Clustering Methods?</h2>
                    The machine learning method of clustering has many different algorithms.
                    <br><br>
                    This page will explain the differences between the main algorithms which are used in this application
                </p> 
            </div>
            <div class="col-12 bg-secondary rounded">
                <p>
                    <h2>K-means</h2>
                    K-Means is the simplest clustering algorithm and just takes the distance between points to determine similarity/dissimilarity.
                    <br><br>
                    At the start of the algorithm depending on the number of clusters specified before hand the same number of centroids are randomly placed on the coordinate space of all points. 
                    These centroids are considered the centre of a cluster. Each the distance between each point and all centroids is calculated. The closest point to each centroid will then be added to that cluster.
                    When all points have been assigned to a cluster, the centroid will then update to be the average position of all points within the cluster. Then the cycle repeats with points being assigned to the closest new centroid and centroid positions being updated. 
                    This stops until a predetermined number of cycles have taken place or when the position of the centroids stays consistent.
                    <br><br>
                    K-means is a good at clustering spherical data i.e. data that forms in the shape of circles/ spheres. However it performs worse when the data is non-spherical or the data is very closely packed together.

                </p>
            </div>
            <div class="col-12 bg-secondary rounded">
                <p>
                    <h2>HDBSCAN</h2>
                    DBSCAN clustered data by finding core points and the point which lie around them. 
                    Core points are high density points, meaning they have many points around them.
                    The number of points and the radius of the vicinity of a point are determined before hand.
                    One core point is then selected as the first cluster, any points close to this core point is then added to the cluster, the same is done for these core points until there are no more core points to add. 
                    If any non-core points are close to these core points they are added to the group.
                    This is then repeated for another randomly selected core point.
                    When there are no more ungrouped core points all remaining points are put as outliers. 
                    <br><br>
                    This is a more complex clustering algorithm; it performs well on non-spherical shapes and clusters of different sizes.
                    Additionally, it provides outlier detection.
                </p>
            </div>
            <div class="col-12 bg-secondary rounded">
                <p>
                    <h2>Agglomerative</h2>
                    Agglomerative clustering is a hierarchical approach.
                    It starts with treating each individual point as its own cluster. 
                    Then a linkage criterion is selected i.e. how to determine which clusters are similar. 
                    This could be linking cluster by the closets point, the furthest point or the average point. 
                    Then these clusters are combined.
                    This continues until a selected number of clusters is attained.
                    Agglomerative clustering has certain advantages like that it provides a dendrogram of the clusters.
                    This can be used to see the different clustering structures at different levels.
                </p>
            </div>
            <div class="col-12 bg-secondary rounded">
                <p>
                    <h2>Bisecting K-means</h2>
                    Bisecting KMeans is another hierarchical approach. 
                    However, it instead takes a top-down approach.
                    Every point starts in one cluster, this cluster is split by KMeans clustering resulting in two clusters.
                    Then the cluster with the most points is again split with KMeans clustering into two more clusters.
                    This is repeated until a target number of clusters is reached.
                    Bisecting KMeans has advantages over other clustering methods as due to it iterative process of cluster splitting it can attain more evenly sized clusters.
                </p>
            </div>
        </div>
    </section>
    
</body>
</html>